---
title: "KmeansR-vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{KmeansR-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(Kmeans)
library(tidyverse)
```
## Introduction to the KmeansR Package
This documentation provides an introduction to the KmeansR package. This package is independent from the Kmeans package available in base R. This package was created as a part of a course project to learn the fundamentals of collaborative software development. 
This package implements the K-Means algorithm for clustering, and provides many additional helpful functions. This will work on any purely numerical dataset with valid features, and includes fit, predict, and cluster_summary functions, as well as as elbow and silhouette methods for hyperparameter “k” optimization.


## Data
To explore the Kmeans package, we will use a randomly generated dataset with 3 clusters. Let's explore how to cluster this dataset using the KmeansR package.

```{r data}
x1 <- rnorm(50, 1, 0.2)
x2 <- rnorm(50, 2, 0.2)
x3 <- rnorm(50, 3, 0.2)
x <- c(x1, x2, x3)
y <- c(x2, x1, x1)

X_train <- data.frame(x, y)
```

## Fitting the data set using `fit()`
Let's fit using the data to get the labels and centers.

```{r fit, fig.width=6, fig.height=5}
km <- fit(X_train, k = 3)
labels <- km$labels
centers <- km$centers

# Plot the clusters
df_results <- X_train
df_results$labels <- labels

ggplot(df_results, aes(x=x, y=y, color=factor(labels))) +
    geom_point() +
    ggtitle("Data after clustering")
```
## Cluster Summary
This gives a summary of the clusters

```{r summary, fig.width=6, fig.height=5}
clustersummary(X_train, centers, labels)
```

## Predicting data given labels
Here we will randomly regenerate the data using the same methods and recluster with the previously found cluster centers.

```{r predict, fig.width=6, fig.height=5}
x1 <- rnorm(50, 1, 0.2)
x2 <- rnorm(50, 2, 0.2)
x3 <- rnorm(50, 3, 0.2)
x <- c(x1, x2, x3)
y <- c(x2, x1, x1)

X_test <- data.frame(x, y)

labels <- predict(X_test, centers)

df_results <- X_test
df_results$labels <- labels

ggplot(df_results, aes(x=x, y=y, color=factor(labels))) +
    geom_point() +
    ggtitle("Data after clustering")
```

## Evaluate different number of clusters sing `elbow()` function

```{r elbow, fig.width=6, fig.height=5}
centers <- c( 1, 2, 3, 4, 5, 6)
el <- elbow(X_train, centers)
inertia <- el$inertia
plot <- el$plot
plot
 
```

The inertia plot has a sharp bend at k=3 suggesting the optimal number of clusters in our dataset is 3.  

## Evaluate different number of clusters using `silhouette()` function

```{r silhouette, fig.width=6, fig.height=5}
centers <- c(2, 3, 4)
sl <- silhouette(X_train, centers)
scores <- sl$scores
plot <- sl$plot
plot
```


As we can see, the silhouette score is the highest at k = 3, which is the optimal number of clusters in out dataset.  
